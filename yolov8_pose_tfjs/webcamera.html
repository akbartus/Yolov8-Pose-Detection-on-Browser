<html lang="en">

<head>
  <meta charset="utf-8">
  <title>YOLOv8 TensorflowJs Serving: WebCamera Pose Detection on the Browser</title>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/tensorflow/4.16.0/tf.js"
    integrity="sha512-h2uoAnR61cun1IpMUeG45l4ZucxXsW265G2ruTZA28qN3SPV2VlE6+34qZYii2JjdxY5vemxH0AHgsw2zoGcNQ=="
    crossorigin="anonymous" referrerpolicy="no-referrer"></script>
  <style>
    .content {
      position: relative;
      width: 400px;
      margin: 0 auto;
    }
    .content>canvas {
      height: 100%;
      left: 0;
      position: absolute;
      top: 0;
      width: 100%;
    }
    button {
      font-size: 20px;
      background-color: #000;
      color: #fff;
      cursor: pointer;
      position: absolute;
      top: 90%;
      left: 50%;
      margin-top: -50px;
      margin-left: -50px;
      width: 150px;
      height: 50px;
    }
    #stopInference {
      display: none;
    }
    button:hover {
      background-color: #fff;
      border: 2px solid #000;
      color: #000
    }
    #header {
      width: 100%;
      text-align: center;
    }
    body {
      -webkit-font-smoothing: antialiased;
      -moz-osx-font-smoothing: grayscale;
      font-family: -apple-system, BlinkMacSystemFont, Segoe UI, Roboto, Oxygen, Ubuntu, Cantarell, Fira Sans, Droid Sans, Helvetica Neue, Helvetica, Arial, sans-serif;
      width: 100%;
    }
    code {
      font-family: source-code-pro, Menlo, Monaco, Consolas, Courier New, monospace
    }
    body {
      margin: 0;
      padding: 0;
      width: 100vw;
      height: 100vh;
      overflow: hidden;
      position: relative;
    }
    .content>video {
      width: 100%;
      border-radius: 10px;
    }
  </style>
</head>

<body>
  <div id="header">
    <h1>YOLOv8 WebCam Pose Detection Example</h1>
    <p>Serving : <code class="code">yolov8n-pose.onnx</code></p>
    <p>Model size : <code class="code">640x640</code></p>
  </div>
  <div id="root">
    <div class="App">
      <div class="content">
        <video id="video" autoPlay="true" playsInline="true" muted="true"></video>
        <canvas id="canvas"></canvas>
      </div>
    </div>
  </div>
  <div>
    <button id="runInference">Run Inference</button>
    <button id="stopInference">Stop Inference</button>
    <script>

      // Add labels if custom
      const labels = [
        "person"
      ];

      // React State implementation in Vanilla JS
      const useState = (defaultValue) => {
        let value = defaultValue;
        const getValue = () => value;
        const setValue = (newValue) => (value = newValue);
        return [getValue, setValue];
      };

      const [loading, setLoading] = useState({ loading: true, progress: 0 });
      const [model, setModel] = useState({
        network: null,
        inputShape: [1, 0, 0, 3],
      });

      const modelName = "yolov8n-pose";
      const modelURL = "model/model.json";

      tf.ready().then(async () => {
        const yolov8 = await tf.loadGraphModel(modelURL, {
          onProgress: (fractions) => {
            setLoading({ loading: true, progress: fractions });
          },
        });
        const dummyInput = tf.ones(yolov8.inputs[0].shape);
        const warmupResults = yolov8.execute(dummyInput);
        setLoading({ loading: false, progress: 1 });
        setModel({
          network: yolov8,
          inputShape: yolov8.inputs[0].shape,
        });
        tf.dispose([warmupResults, dummyInput]);
      });

      // Preprocess image
      function preProcessImage(source, modelWidth, modelHeight) {
        let widthRatio, heightRatio;
        const input = tf.tidy(() => {
          const img = tf.browser.fromPixels(source);
          const [h, w] = img.shape.slice(0, 2);
          const maxSize = Math.max(w, h);
          const imgPadded = img.pad([
            [0, maxSize - h],
            [0, maxSize - w],
            [0, 0],
          ]);
          widthRatio = (maxSize / w) * 1.85;
          heightRatio = maxSize / h;

          return tf.image
            .resizeBilinear(imgPadded, [modelWidth, modelHeight])
            .div(255.0)
            .expandDims(0);
        });
        return [input, widthRatio, heightRatio];
      };

      // Detect 
      async function detect(source, model, canvasRef, callback = () => { }) {
        const [modelWidth, modelHeight] = model.inputShape.slice(1, 3);
        const [input, widthRatio, heightRatio] = preProcessImage(source, modelWidth, modelHeight);
        const predictions = model.network.execute(input);
        const transpose = predictions.transpose([0, 2, 1]);
        const boxes = tf.tidy(() => {
          const w = transpose.slice([0, 0, 2], [-1, -1, 1]);
          const h = transpose.slice([0, 0, 3], [-1, -1, 1]);
          const x1 = tf.sub(transpose.slice([0, 0, 0], [-1, -1, 1]), tf.div(w, 2));
          const y1 = tf.sub(transpose.slice([0, 0, 1], [-1, -1, 1]), tf.div(h, 2));
          return tf.concat(
            [
              y1,
              x1,
              tf.add(y1, h),
              tf.add(x1, w),
            ],
            2
          ).squeeze();
        });

        const scores = tf.tidy(() => {
          const rawScores = transpose.slice([0, 0, 4], [-1, -1, 1]).squeeze();
          return rawScores;
        });
        const landmarks = tf.tidy(() => { return transpose.slice([0, 0, 5], [-1, -1, -1]).squeeze(); });
        const nms = await tf.image.nonMaxSuppressionAsync(boxes, scores, 500, 0.45, 0.3);
        const boxes_data = boxes.gather(nms, 0).dataSync();
        const scores_data = scores.gather(nms, 0).dataSync();
        let landmarks_data = landmarks.gather(nms, 0).dataSync();
        landmarks_data = tf.reshape(landmarks_data, [-1, 3, 17]);
        drawLabels(source, canvasRef, landmarks_data, boxes_data, scores_data, widthRatio, heightRatio);
        tf.dispose([predictions, transpose, boxes, scores, nms]);
      };




      const detectVideo = (vidSource, model, canvasRef) => {

        const detectFrame = async () => {
          detect(vidSource, model, canvasRef, () => {
            requestAnimationFrame(detectFrame); // get another frame
          });
        };

        detectFrame();
      };

      // Colors and landmarks
      const colors = {
        nose: "cyan",
        leftEye: "lime",
        rightEye: "magenta",
        leftEar: "yellow",
        rightEar: "dodgerblue",
        leftShoulder: "hotpink",
        rightShoulder: "orange",
        leftElbow: "turquoise",
        rightElbow: "springgreen",
        leftWrist: "deeppink",
        rightWrist: "chartreuse",
        leftHip: "skyblue",
        rightHip: "coral",
        leftKnee: "mediumspringgreen",
        rightKnee: "deepskyblue",
        leftAnkle: "orangered",
        rightAnkle: "gold"
      };

      const connections = [
        ["nose", "leftEye"],
        ["nose", "rightEye"],
        ["leftEye", "leftEar"],
        ["rightEye", "rightEar"],
        ["leftShoulder", "rightShoulder"],
        ["leftShoulder", "leftElbow"],
        ["rightShoulder", "rightElbow"],
        ["leftElbow", "leftWrist"],
        ["rightElbow", "rightWrist"],
        ["leftShoulder", "leftHip"],
        ["rightShoulder", "rightHip"],
        ["leftHip", "rightHip"],
        ["leftHip", "leftKnee"],
        ["rightHip", "rightKnee"],
        ["leftKnee", "leftAnkle"],
        ["rightKnee", "rightAnkle"]
      ];

      function drawLabels(source, canvasRef, landmarks_data, boxes_data, scores_data, widthRatio, heightRatio) {
        const ctx = canvasRef.getContext("2d");
        ctx.clearRect(0, 0, ctx.canvas.width, ctx.canvas.height);
        //ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
        for (let i = 0; i < scores_data.length; ++i) {
          const score = (scores_data[i] * 100).toFixed(1);
          let [y1, x1, y2, x2] = boxes_data.slice(i * 4, (i + 1) * 4);
          x1 *= widthRatio;
          x2 *= widthRatio;
          y1 *= heightRatio;
          y2 *= heightRatio;
          const width = x2 - x1;
          const height = y2 - y1;
          ctx.fillStyle = colors["nose"];
          ctx.strokeStyle = colors["nose"]
          ctx.lineWidth = Math.max(Math.min(ctx.canvas.width, ctx.canvas.height) / 200, 2.5);
          ctx.strokeRect(x1, y1, width, height);
          let keypoints = landmarks_data.slice([i, 0, 0], [1, -1, -1]).reshape([17, 3]).arraySync();
          ctx.lineWidth = 2;
          ctx.strokeStyle = "white";

          // Draw Connections
          for (const [partA, partB] of connections) {
            const x1 = keypoints[Object.keys(colors).indexOf(partA)][0] * widthRatio;
            const y1 = keypoints[Object.keys(colors).indexOf(partA)][1] * heightRatio;
            const x2 = keypoints[Object.keys(colors).indexOf(partB)][0] * widthRatio;
            const y2 = keypoints[Object.keys(colors).indexOf(partB)][1] * heightRatio;
            ctx.beginPath();
            ctx.moveTo(x1, y1);
            ctx.lineTo(x2, y2);
            ctx.stroke();
            ctx.closePath();
          }
          // Draw Keypoints
          for (let j = 0; j < keypoints.length; j++) {
            const x = keypoints[j][0] * widthRatio;
            const y = keypoints[j][1] * heightRatio;
            const bodyPart = Object.keys(colors)[j];
            ctx.beginPath();
            ctx.arc(x, y, 2.5, 0, 2 * Math.PI);
            ctx.fillStyle = colors[bodyPart];
            ctx.fill();
            ctx.closePath();
          }
        }
      }

      // Run inference
      document.querySelector("#runInference").addEventListener("click", () => {
        const video = document.querySelector("#video");
        const canvas = document.querySelector("canvas");
        const context = canvas.getContext("2d");
        video.style.display = "block";

        // Set video stream constraints
        const constraints = {
          audio: false,
          video: { width: 640, height: 640, facingMode: "environment" },
        };

        // Request access to the user"s camera
        navigator.mediaDevices
          .getUserMedia(constraints)
          .then((stream) => {
            video.srcObject = stream;
            video.play();
            setInterval(() => {
              detectVideo(video, model(), canvas);
            }, 100);
          })
          .catch((err) => {
            console.error(err);
          });

          setTimeout(() => {
                    document.querySelector("#stopInference").style.display = "block";
                }, 2000);
  
      })

            // Stop inference
            document.querySelector("#stopInference").addEventListener("click", () => {
                const video = document.querySelector("#video");
                video.style.display = "none";
                let stream = video.srcObject;
                stream.getTracks().forEach(function (track) {
                    track.stop();
                })
                setTimeout(() => {
                    document.querySelector("#stopInference").style.display = "none";
                    document.querySelector("#runInference").style.display = "block";
                }, 2000);


            })
    </script>
</body>
</html>